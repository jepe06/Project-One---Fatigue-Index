
\documentclass[conference]{IEEEtran}
% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
\usepackage{float}  % For handling float environments like [H]
\usepackage{url}    % For handling URLs
\usepackage{multirow}
\usepackage{subcaption} % Required for subfigures
\usepackage{booktabs}


\usepackage{cite}
\pagestyle{plain}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

% *** MATH PACKAGES ***
%
\usepackage{amsmath}

% *** FLOAT PACKAGES ***
%
\usepackage{float}
%\usepackage{url}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}


\title{Applying ML to assess fatigue and prevent injury\\in high performance swimming athletes}

% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Hugo Veríssimo}
\IEEEauthorblockA{Foundations of Machine Learning 24/25\\
University of Aveiro\\
Aveiro, Portugal\\
hugoverissimo@ua.pt}
\and
\IEEEauthorblockN{João Cardoso}
\IEEEauthorblockA{Foundations of Machine Learning 24/25\\
University of Aveiro\\
Aveiro, Portugal\\
joaopcardoso@ua.pt}}


% make the title area
\maketitle
\thispagestyle{plain}

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
temos que arranjar refs para ir metendo por ai
\end{abstract}


\IEEEpeerreviewmaketitle


\section{Introduction}
The evolution in performance of high level athletes is highly dependent on their skill, motivation, and discipline. With the support of a knowledgeable coach, the evolution can be substantially improved, through careful tailoring of the training regimen. One of the most relevant metrics since the dawn of structure training in sports is the feedback from the athlete, commonly described as the rate of perceived exertion (RPE). This single metric comprises the athletes analysis and intuition of the effort that was carried and how ready they feel for another bout of training. In recent years, more and more sports coaches have relied heavily on collected data to better assess, plan and adjust the training plans of their athletes in a systematic way. This allows for a fine balance between intense workouts, that generate stronger stimuli for muscle development and sport specific skills, taking the balance between effort and fatigue as the crucial ratio to respect. Too high effort, may lead to injury, too low and some gains may be left on the table. 

In the scope of the first project for Foundations of Machine Learning, we decided to partner with the local sports club CAPGE (Clube Associação de Pais da Gafanha da Encarnação) to treat the data (kindly shared by Sr Coach Daniel Tavares) for estimating the fatigue of several athletes. The data was curated and prepared to implement and fit several machine learning algorithms to estimate fatigue after workout.

With this approach, we aim to generalize the models for different athletes/sports, and make it available to the local club for implementation and further testing. 



\section{State of the Art}

Over the past decade there have been significant improvements in the field of ML applied algorithms for sports' related applications. Here we present the most relevant work in fatigue and injury prediction, which is not specifically on the sport we're assessing in our dataset, which lead to additional interpretations from the works analyzed to our own case of study. In general, the problem of class imbalance is seen throughout the literature, and different solutions are proposed, such as data gathering and preprocessing, over sampling and under sampling, with SMOTE (synthetic minority over-sampling technique) being the most commonly used approach for over sampling. As early as 2010, Gabbet and colleagues modeled the risk of injury with a monodimensional approach using logistic regression, based on athletes rate of perceived exertion, showing that even with a monovariate approach to injury prediction useful results could be attained. In recent years, several authors have focused in alternative techniques such as Logistic Regression, Random Forest, Support Vector Machine, or Convolutional Neural Network on Multivariate Timeseries (the references for these papers can be found in the paper "A Narrative Review").
Besides model selection, feature engineering and selection is among the most debated topics. Several authors opt to include GPS data, metabolic consumption, mechanical load, RPE, detailed quantification of workloads, ratio between acute:chronice loads. Despite the multivariate imputation, data analysis often shows strong correlations between them, leading to over fitting problems (usually model independent).
In the work by Carey et al. (2018) different algorithms have been implemented to predict the risk of injury in an Australian football club. The data collection lasted for three seasons, consisting of absolute and relative training load metrics, derived from GPS, accelerometer, and RPE data. The prediction models used were regularized logistic regression, generalized estimating equations, random forests, and support vector machines, with periods of 3, 6, and 21 days (these periods have been studied and verified as adequate for the case of Australian football). The periods served to calculate moving averages and exponentially weighted moving averages (EWMA). The latter allowed to account for the decay in significance of the training load the further it happened from a given day, in accordance with the work from Williams et al. (2016). From the results it was possible to verify that over fitting was very likely due to the multicollinearity between variables, which was confirmed by principal component analysis (PCA). The use of PCA with regularized logistic regression slightly improved the results.
More recent studies have employed ensemble algorithms, in order to take most of the different learning models selected, taking into account the need to balance the classes as is common practice for this type of problems. 
In summary, the integration of machine learning techniques in sports fatigue and injury prediction has evolved from simple monovariate models to complex multivariate and ensemble approaches. Addressing challenges such as class imbalance, feature selection, and multicollinearity remains crucial for developing robust predictive models applicable across different sports contexts.


\section{Dataset Analysis}
\subsection{Data Description}

The data used in this project was collected from the swimming club CAPGE during the season of 2019/2020, where each athlete has several observations corresponding to training days, where each of the features was collected. Not all athletes logged the same number of training days, nor present an equal distribution between low, average and high levels of intense training. The names of the athletes were removed to ensure privacy and confidentiality, keeping only the gender as a variable. The team is comprised by seven athletes, three male and four female. Most of the features are related to feedback from the athletes on different aspects of their lives (i.e. sleep quality, appetite, and rate of perceived exertion after training), while others are measurable (i.e. workload, variation in heart rate before and after training, weight variation). A notable feature to mention is the RPE, that is still deemed as one of the most relevant metrics for workload planning and fatigue assessment. All these attributes are classified between 1 — 10, each value corresponding to increasingly 'worse' categories (e.g., 1 great appetite / normal, 10 no appetite at all).
The fatigue index is calculated from these features, using weights attributed by the coach based on his empirical experience. The resulting fatigue index is between 0 — 100, which was categorized in four classes as seen in Table 1. 


\begin{table}[H]
\centering
\caption{Sample Table with Ranges and Classifications}
\label{classTable}
\begin{tabular}{|c|c|c|}
    \hline
    \textbf{Range} & \textbf{Initial Classes} & \textbf{Final Classes} \\ \hline
    $>$ 90 & Risk    & \multirow{2}{*}{Risk/Caution} \\ \cline{1-2}
    $>$ 80 & Caution & \\ \hline
    $\geq$ 40 & Optimal & Optimal \\ \hline
    $<$ 40  & Low     & Low \\ \hline
\end{tabular}
\end{table}

There is a big gap between fatigue classes due to the nature of training and performing high effort workouts in specific times of the training cycle. The dataset was provided in Excel format (per athlete), from which we imported and combined the data as a pandas dataFrame to apply the different models.



\subsection{Dataset curation}

The initial assessment evidenced the need for balancing our data. To start, we've reduced the number of classes, by combining the two higher risk classes ('Caution' and 'Risk'). With this, the number of observations was closer between 'Low/Minimal' and 'Risk/Caution', leaving us with an excess of observations for Optimal, as seen in Figure. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/distribution_FatigInd(0).png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{histClasses}
\end{figure}

At this stage, we opted to under sample our dataset to the number of observations of 'Risk/Caution', and over sample the observations in 'Low/Minimal', by imputing random samples from the pool of observations of 'Low/Minimal'. The use of SMOTE in this scenario would provide continuous classes for our features, which wouldn't yield any physical meaning.
To assess how the different features vary among them and in relation to the target, we computed the correlation matrix as seen in Figure \ref{fig:correlationMatrix}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/correlation_matrix.png}
    \caption{hgfdgushfg}
    \label{fig:correlationMatrix}
\end{figure}
From this we could exclude several of the features, which are verified by how the classes are distributed across the scales for each feature. Figure illustrates a proper and poor example of class distribution for a given feature.
\begin{figure}
    \centering
    % First subfigure
    \begin{subfigure}[1]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/distribution_pEffort.png}
        \caption{Perceived effort distribution.}
        \label{fig:subfig-a}
    \end{subfigure}
    
    % Add some vertical space between the figures if needed
    \vspace{0.5cm}

    % Second subfigure
    \begin{subfigure}[2]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{assets/distribution_StSpirit.png}
        \caption{Perceived effort distribution.}
        \label{fig:subfig-b}
    \end{subfigure}
    
    \caption{Class distributions across various metrics: (a) Perceived effort, (b) Another metric.}
    \label{fig:main-figure}
\end{figure}

Considering that the weights used in the dataset were identical regardless of sex, we performed some simple models in order to decide if it would be necessary to split our dataset. We could verify that gender didn't have a significant impact in model performance, so we opted to use it as a feature.
In order to keep the timeseries nature of training, increase and decrease of training intensity, and varying fatigue with training we included exponentially weighted moving averages (EWMA). This way we include memory in our model, and allow to consider the decay in the weights of events further away from any given day. The mathematical expression used to calculate EWMA for each selected feature is,
\begin{equation*}
    \text{EWMA}_{\text{today}} = \text{Feature}_{\text{today}} \cdot \lambda_a + (1 - \lambda_a) \cdot \text{EWMA}_{\text{yesterday}}
\end{equation*}
where $\lambda_a$ is a value between 0 and 1 that represents the degree of decay, with higher values discounting older observations at a faster rate. The $\lambda_a$ is given by:
\begin{equation*}
    \lambda_a = \frac{2}{N + 1}
\end{equation*} 


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/distribution_FatigInd(F_f).png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{histClasses}
\end{figure}

tentamos o smote mas ele criava variaveis continuas mesmo quanto eram categoricas entre outros problemas e por nao precisarmos de uma grande qnt relativa de obs geradas, ent usou se o metodo de "copiar obervacoes (procurar nome melhor)"

ONDE SE EMTE O GRAFICO DAS CLASSES EQUILIBRADAS?



\subsection{analisa das features}

fazer descricao das feautures realmente usadas ?

ns q temos q standartizar os dados para se tornar mais facil a sua analise, para q esteja tudo na mesma medida bla bla

OU PASSAR ISSO TUDO PARA O CLASSIFICATION MODELS?

\section{classifcation models}

para classificar a fadiga, vamos usar varios modelos de machine learning, aplicados através da liguangem de programacao python, pelo sua extensivas livrarias com funcoes dedicadas a este tipo de analise. nomeadamente reg logistica (\texttt{LogisticRegression}), support vector machine (\texttt{SVC}) e arvore de decisao (\texttt{DecisionTreeClassifier}), e exprimentar variacoes dos mesmos, tendando encontrar o que melhor modela os nossos dados, utilizando a funcao de otimizacao de modelos \texttt{RandomizedSearchCV}, com um cross validation de 8 para diminuir o risco de overfitting

uma componente mt importante deste estudo é o avaliar cada modelo, para isso serão usadas metricas tai como ...., para analisar modelo a modelo qual é o melhor para modelas/classificar os vários niveis de fadiga

mostrar algumas formulas e graficos de exemplo etc etc

para alem disso, importante referir q cada modelo sera treinado com $80\%$ dos dados e bla bla \texttt{train\_test\_split}.


\section{regressao logistica}

AUSHDIUAHIAHFSIH


\subsection{sem termo de castigo ou assim}

ns q resultados bla bla

\subsection{com termo de castigo ou assim}

ns q mais resultados

meter tabelas com resultados e tudo mais, mas so no fim metemos uma geral a comparar tudo

meter:

- matriz de confusao

- precision score

- f1 score

- accuracy score

- recall score

- cross validation

\section{Support Vector Machine}

para a otimizacao do modelo, foram testadas varias combinacoes de parametros diferentes, nomedamente o Regularization Parameter ($1/\lambda$), o Kernel Coefficient ($\gamma$), o tipo de kernel, o grau máximo caso para o kernel do tipo `poly` (degree) e o valor do termo indepentende (Coef$_0$), podendo-se obsevar então um resumo? destas combinacoes na tabela \ref{parametrosSVM}.

\begin{table}[H]
\centering
\caption{SVM Hyperparameter Search Space ?}
\label{parametrosSVM}
\begin{tabular}{ll}
\toprule
Parameter & Distribution/Values \\
\midrule
$1/\lambda$ & Uniform$(0, 100)$ \\ 
$\gamma$ & \{\text{scale, auto, $0.1$, $0.01$, $0.001$}\} \\ 
Kernel & \{\text{linear, rbf, poly, sigmoid}\} \\ 
Degree & \{1, 2, 3\} \\ 
Coef$_0$ & \text{Uniform}(-5, 5) \\
\bottomrule
\end{tabular}
\end{table}

contudo, ao aplicar o melhor modelo obtido tendo em conta as combinacoes destes pametros, os resultados nao foram satisfatorios, tendo em conta q a classe optimal estava mt má

para isso foi decidido aplicar pesos às classes, fixando o peso de todas as classes com excecao da optimal, sendo este considerado agora como um hyperparemto

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/SVM_OptimalWeight.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{svm_weight}
\end{figure}

através da dafiura \ref{svm_weight}, verifica-se que o peso ótimo apra esta classe é $0.8$, quando as restantes classes têm peso 1.

aplicando novamente a otimizacao do modelo SVM , tendo em conta estes pesos, foram obtidos resultados mais satisfatorios

QUAL FOI OS HYPERPAREMTES OBTIDOS

$$Best parameters found:  {'C': np.float64(6.9254306285859935), 'coef0': np.float64(-0.2008674096203551), 'degree': 3, 'gamma': 'auto', 'kernel': 'rbf'}$$

alguns ele n usa, tp o degree é ignorado, o coef0 se calhar tbm, ver melhor isso

\subsection{resultados de treino}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/SVM_LearningCurve.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{svm_learningcurve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/SVM_ConfusionMatrixTrain.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{svm_cm_train}
\end{figure}

\begin{table}[H]
\centering
\caption{Classification Report for Model Evaluation (treino)}
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Low/Minimal & 0.84 & 0.89 & 0.86 & 170 \\
Optimal & 0.81 & 0.54 & 0.65 & 170 \\
Risk/Caution & 0.67 & 0.85 & 0.75 & 171 \\
\midrule
\textbf{Accuracy} &  &  & 0.76 & 511 \\
\textbf{Macro avg} & 0.77 & 0.76 & 0.75 & 511 \\
\textbf{Weighted avg} & 0.77 & 0.76 & 0.75 & 511 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{resultados de teste}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/SVM_ConfusionMatrixTest.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{svm_cm_test}
\end{figure}

\begin{table}[H]
\centering
\caption{Classification Report for Model Evaluation (teste)}
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Low/Minimal & 0.74 & 0.93 & 0.82 & 43 \\
Optimal & 0.76 & 0.44 & 0.56 & 43 \\
Risk/Caution & 0.73 & 0.86 & 0.79 & 42 \\
\midrule
\textbf{Accuracy} &  &  & 0.74 & 128 \\
\textbf{Macro avg} & 0.75 & 0.74 & 0.72 & 128 \\
\textbf{Weighted avg} & 0.75 & 0.74 & 0.72 & 128 \\
\bottomrule
\end{tabular}
\end{table}


\section{decision tree model}

os paraemtros a otimizar e varias e bla bla Critério de divisão, Profundidade máxima da árvore, Número mínimo de amostras para dividir um nó e Número mínimo de amostras em um nó folha

\begin{table}[H]
\centering
\caption{DTREE Hyperparameter Search Space ?}
\label{parametrosDTree}
\begin{tabular}{ll}
\toprule
Parameter & Distribution/Values \\
\midrule
Split Criterion & \{gini, entropy\} \\ 
Max Depth & $[2, \dots, 8]$ \\ 
Min Samples to Split & $[5, \dots, 20]$ \\ 
Min Samples per Leaf & $[3, 4, \dots, 10]$ \\
\bottomrule
\end{tabular}
\end{table}

QUAL FOI OS HYPERPAREMTES OBTIDOS

$$Best Parameters: {'splitter': 'best', 'min_samples_split': np.int64(11), 'min_samples_leaf': np.int64(7), 'max_depth': np.int64(4), 'criterion': 'entropy'}$$

otimiada a arvore, foi obtida a seguinte estrutua

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/DTREE_Tree.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{dtree_tree}
\end{figure}

verifica-se que foram o depht é de 4, as cores representam a classe (lanrnja tal, verde tal, ...)

\subsection{resultados de treino}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/DTREE_LearningCurve.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{dtree_lcurve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/DTREE_ConfusionMatrixTrain.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{dtree_cmtrain}
\end{figure}

\begin{table}[H]
\centering
\caption{Classification Report for Model Evaluation (treino)}
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Low/Minimal & 0.82 & 0.74 & 0.77 & 170 \\
Optimal & 0.52 & 0.62 & 0.57 & 170 \\
Risk/Caution & 0.69 & 0.63 & 0.65 & 171 \\
\midrule
\textbf{Accuracy} &  &  & 0.66 & 511 \\
\textbf{Macro avg} & 0.68 & 0.66 & 0.67 & 511 \\
\textbf{Weighted avg} & 0.68 & 0.66 & 0.67 & 511 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{resultados de teste}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/DTREE_ConfusionMatrixTest.png}
    \caption{Histogram of the new classes, once 'Caution' and 'Risk' are combined in one.}
    \label{dtree_cmtest}
\end{figure}

\begin{table}[H]
\centering
\caption{Classification Report for Model Evaluation (teste)}
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Low/Minimal & 0.72 & 0.77 & 0.74 & 43 \\
Optimal & 0.47 & 0.47 & 0.47 & 43 \\
Risk/Caution & 0.64 & 0.60 & 0.62 & 43 \\
\midrule
\textbf{Accuracy} &  &  & 0.61 & 128 \\
\textbf{Macro avg} & 0.61 & 0.61 & 0.61 & 128 \\
\textbf{Weighted avg} & 0.61 & 0.61 & 0.61 & 128 \\
\bottomrule
\end{tabular}
\end{table}

\section{comparacao dos modelos}

comparar aqui todos os resultados e isso

acuracaia, learning curves, etc , etc

- learning curves

- scalability 

- performance 

- cross validation

\section{restulados}

observa se q o melhor é tal e tal, mas aquele fez isto e ns q


\section{Conclusion}
The conclusion goes here.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}









% use section* for acknowledgment
\section*{Acknowledgment}


%The authors would like to thank... ~\cite{Author2024}


\section{NOTAS}

Considerar usar dataset com informação combinada de atletas para reter carácter temporal (progressão da época, aumento de cargas e combinação com features que tenham referência temporal [carga de treino do dia anterior]

Modelos sugeridos: Random Forest / SVM

temos com objetivo generalizar para que qq treinador possa ter nocao do comporamento e do estado dos seus atletas, podendo ajustar os seus treinos e cargas físicas consoante as medidas de fadigas.

para alem disso tbm se quer ver quais as metricas mais importantes relacionadas coma a fadiga

De acordo com a conversa com a professora é importante perceber como é que se deve definir a memória da nossa t-SNE, e a importância que isso tem no período da fadiga.

ns q dados fornecidos por um treinador de natacao, tivemos de organizar os dados em folhas de excel, visto estarem por linhas e com graficos e formulas de acordo com o treinador, bla bla, teve-se fazer oq? sabes melhor q eu pq foste tu q fizeste

os valores da fadiga foram convertidos para categoricos, pq e mais interessante classificar a fadiga, tendo em conta intervalos dados pelo treinador, do q numero que tornam mais dificl a interpretacao dos mesmos

de seguida agruparam-se os dados todos num novo ficheiro excel, para posterior analise atraves do python, de forma mais facilitata

tem se entao dados diarios, para X atletas, durante ns q tempo, ao longo da epoca tal, totalizando x observacoes (linhas)

para cada observacao tem se variaveis como ... bla bla e bla, contudo, nem todas serao usadas, devido a insights dados pelo treinador aka expert

as variaveis que irao ser utilizadas no estudo são tal tal, que representa tal, tal tal, ... ns q ns q mais.




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


\bibliographystyle{IEEEtran}
\bibliography{references}
\begin{thebibliography}{99}
\bibitem{example} A. Author, "Title of the paper," Journal Name, vol. 12, no. 3, pp. 45--67, 2024.
\end{thebibliography}

\end{document}



